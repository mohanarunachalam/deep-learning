{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">TensorFlow Neural Network Lab</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/notmnist.png\">\n",
    "In this lab, you'll use all the tools you learned from *Introduction to TensorFlow* to label images of English letters! The data you are using, <a href=\"http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\">notMNIST</a>, consists of images of a letter from A to J in different fonts.\n",
    "\n",
    "The above images are a few examples of the data you'll be training on. After training the network, you will compare your prediction model against test data. Your goal, by the end of this lab, is to make predictions against that test set with at least an 80% accuracy. Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start this lab, you first need to import all the necessary modules. Run the code below. If it runs successfully, it will print \"`All modules imported`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notMNIST dataset is too large for many computers to handle.  It contains 500,000 images for just training.  You'll be using a subset of this data, 15,000 images for each label (A-J)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Download data and load. The one_hot=True means that the labels are \"one hot\" encoded vector instead of actual digit \n",
    "#associated with the image. For example in place of label is being \"2\" you will see [0,0,1,0,0,0,0,0,0,0]. This allows \n",
    "#us to directly use in output layer of the network \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the pixels in the image (28 * 28 = 784)\n",
    "features_count = 784\n",
    "# there are 10 digits i.e labels\n",
    "labels_count = 10\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learning_rate = 0.5\n",
    "\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None,features_count])\n",
    "labels = tf.placeholder(tf.float32, [None, labels_count])\n",
    "\n",
    "#Set the weights and biases tensors\n",
    "weights = tf.Variable(tf.truncated_normal((features_count, labels_count)))\n",
    "biases = tf.Variable(tf.zeros(labels_count),name='biases')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Linear Function WX + b\n",
    "logits = tf.add(tf.matmul(features, weights),biases)\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy\n",
    "cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "# Training loss\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# this initializes all variables\n",
    "init = tf.global_variables_initializer()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    \n",
    "\n",
    "\n",
    "# Determine if the predictions are correct\n",
    "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 1.267\n",
      "Epoch: 2 cost = 0.583\n",
      "Epoch: 3 cost = 0.492\n",
      "Epoch: 4 cost = 0.443\n",
      "Epoch: 5 cost = 0.413\n",
      "Epoch: 6 cost = 0.391\n",
      "Epoch: 7 cost = 0.376\n",
      "Epoch: 8 cost = 0.360\n",
      "Epoch: 9 cost = 0.352\n",
      "Epoch: 10 cost = 0.341\n",
      "0.9084\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "   # initialise the variables\n",
    "   sess.run(init)\n",
    "   total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "   for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c = sess.run([optimizer,loss], feed_dict={features: batch_x, labels: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "   print(sess.run(accuracy, feed_dict={features: mnist.test.images, labels: mnist.test.labels}))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
